07-14
=====
Writing the test sequence for the optimizer. This will pipeline testing.
The idea is to just have to run this nice ``conduct_experiment'' function that will simply collect all the relevant statistics about a run of 100 optimizations (or some other configurable number) on many different objective functions. Such an experiment might take up to 10 minutes to run, and we need to ensure a way to reproduce these experiments reliably. It seems to me that the simplest way of doing so would be to clone the source code of the optimizer (dm_optimizer.py) into the experiment folder, as well as provide a way to dump the configuration of the optimizer. Doing so, it will become possible to definitely reproduce any past experiment.

07-15
=====
Reproducibility is still not completely achieved by the test suite, but using it I have identified a number of very problematic functions, on which the optimizer fails pretty consistently. In fact, on 11 objective functions, there are 5 on which the failure rate is >95%. (Note: this is not just 1 - success_rate; the failure rate is really the ratio of optimizer runs that *failed* abnormally.) I suspect this is due to the fix I introduced earlier which causes the optimizer to noisily die if it does not have sufficient minima to proceed. Furthermore, these functions seem to be the kind that the optimizer would solve immediately, by starting by chance in the neighbourhood of the global minimum. Indeed, the cigar function and sphere function are such functions, and their failure rates are 100%. The idea is this: the optimizer picks two starting points, both lead to the same minimum (the global minimum, probably), and then the optimizer dies because it requires that there be at least two distinct minima to proceed with the optimization. As for the functions with high but non-100% failure rates, I suspect that their high failure rate arises because they have few local minima. Therefore, the probability of the optimizer picking distinct minima is low, but nonzero, which explains the failure rates of 98% and 96% for the h1 function and rosenbrock functions, respectively. From the graphs of these function in 3D, I can see that the neighbourhood of the global minimum is very large.
The fix that I propose for this is rather than to report a failure status when the optimizer encounters such a case, it can instead exit "successfully", simply reporting the value that it has found immediately. Of course, its exit message in that case will be something along the lines of "Local minima exhausted," and its exit code will be nonzero, such that any statistics-collecting software can appropriately report such cases.
Another important thing to add to the test suite would be multiprocessing. This would allow the testing to proceed about four times faster by exploiting multiple cores of the processor, one for each objective function. Race conditions should not occur because the processes would share no data, and would never access the same files.

Introducing the fix proposed above -- changing the exhaustion of local minima into a success condition -- did not reduce the failure rates of the functions mentioned above. This doesn't actually make sense, since failure is measured by counting the runs whose success attribute is False.

Examining the exit messages of the runs proved insightful, and showed that failures are originating from many other places. I managed to suppress some of the failures originating from an exception that used to be thrown when the deltay_curr was too small (instead, I set it to a kind of pseudo).

I tried implemented mutliprocessing to increase the speed, by running 4 optimizations simultaneously. However, this requires passing functions to the child processes (namely the objective functions), and that's done with pickle, which can't deal with functions. Since I can just load the objective functions from DEAP though, it might be worthwhile to just pass the name of the function, and from that I can load it dynamically (yay for interpreted languages, for once). This will greatly increase the speed of the testing.

Tomorrow I need to track down the other sources of failures by examining the exit messages of the tests, and I need to fix multiprocessing. I also need to generalize / clone ``conduct_all_experiments'' to work on simulated annealing as well, that way I can compare them. As for fair comparison, I should try to look into something like scaling the time taken by the success rate, or something.

07-16
=====
Failures fixed. Simply swallowing the exeptions and returning the best value so far results in improved success. I feel that simply swallowing exceptions and bailing out is pretty shitty though. It would be nice if there were some clever way of continuing / avoiding these situations in the first place. Of course, I suspect that these situations arise as a result of the deletion technique. What happens to performance if we play with the refresh rate ?

Now that I think of it, actually, here, I'm checking for success only by looking at the function value. Let's suppose that there is a local minimum whose function value is below the threshold but not truly equal to the optimal value. In that case, I'm counting this run as a success, despite it not being *at the right place* (on the x-axis). I could include a check on the x-coordinate as well, but some objective functions have funny x-coordinates that depend on the dimensions. It would require restructuring a decent chunk of the test framework... I think that in practice, it's *okay*. So what, we don't find the true global minimum, but we have instead some value that's right next to it and whose score on the objective function is still pretty goddamn low. Practically speaking, such a point is pretty good for whatever purpose we want.

Performance for a refresh rate of 8 decreases overall to 55.7%. It seems that Ackley suffered the most. The Simon function #2 (which is atrociously slow), has an extremely low success rate of 0.8%, and is also the function with the highest average runtime. Increasing the refresh rate to 20 brings the global success rate down to 54%. This seems to just be the effect of Ackley's performance dropping. The other functions don't change in a statistically significant way, so it seems.

07-17
=====
Since we want to extend this version control scheme that I have crafted to guarantee reproducibility, I need to generalize my run_scripts.py. First, I'll add a separate module that it loads that contains the list of files to watch, ``reproducible.py''. Then, I'll rename run_tests.py to run_reproducible.py. Simply, load a module specified on the command line, and execute that module's conduct_all_experiments or run function. That function must return a directory where run_reproducible can store the hash of the commit hash.

At the end of the day, everything seems in place. run_reproducible has its own github repository now https://github.com/djeik/reproducible. I didn't get much work done on the optimizing the optimizer proper (heh). On the other hand, some fixes I seem to have introduced a while back have increased the runtime (by a LOT). This is probably due to generalizing the code to allow for SA. This involves a lot of dictionaries and lambdas, which AIUI reaaaaaaally slow. Thanks to my amazing accounting powers and experimental data, I can lookup exactly where this happened and figure out how to fix it. That, however, is for tomorrow.

Holy shit, I wasn't kidding when I said really slow. IT'S INSANELY SLOW. Around 23 minutes for DM!!! It's certainly the dicts and lambdas, but still. That's madness. It was taking around 250 seconds before. This is a ~5x increase! Tomorrow will be a day of much profiling.

07-18
=====
No real work on the optimizer today, but discussed with Simon different approaches for improving it:
* Look into cooling schedules: it's something we'd like to avoid, but if it turns out to really make DM shine, then it might be worthwhile.
* Use more than one target value: by maintaining several target values, we can try to make several steps and check each one locally. By examining each one and using a configurable decision procedure, we can get a more holistic view of the search space and try to make smarter jumps.
* Look into those unrecoverable scenarios, and try to make them recoverable. This might only be fixable by playing with cooling schedules, and our procedure for dropping past minima.

07-23
=====
No real work on the optimizer today, but more discussion of future directions with Simon.
* use SA as a benchmark to determine what's the best dimension for testing each objective function
* try playing with the number of iterations in SA: do higher counts necessarily improve it? (For DM, playing with the counts is basically unnecessary past some threshold around 250, it seems.)
* try fancy "sniffing before digging" methods, like line searches and checking what happens if we step in many different directions, based on more than one past local minimum.
Really weird bug... seems like only three iterations are recorded out of the 250 ! Is seems that this bug was introduced when I switched to the generalized system in which I can test DM and SA uniformly...

07-24
=====
Tracked down that weird bug. It meant writing out a lot of for loops and bashing my head on the wall, but it seems to work now. There's something weird going on: the optimizer fails pretty frequently, but the averages are calculated only on the number of runs that produced some output.... so we get 100% success on the sphere function, but in reality, only 86/250 runs actually did something ! I think I need to relax those conditions on the starting positions, or do something in any case so that every run at least does something. That's one of the features of our method: it is capable of continuing to search essentially forever.
