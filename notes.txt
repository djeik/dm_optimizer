07-14
Writing the test sequence for the optimizer. This will pipeline testing.
The idea is to just have to run this nice ``conduct_experiment'' function that will simply collect all the relevant statistics about a run of 100 optimizations (or some other configurable number) on many different objective functions. Such an experiment might take up to 10 minutes to run, and we need to ensure a way to reproduce these experiments reliably. It seems to me that the simplest way of doing so would be to clone the source code of the optimizer (dm_optimizer.py) into the experiment folder, as well as provide a way to dump the configuration of the optimizer. Doing so, it will become possible to definitely reproduce any past experiment.

07-15
Reproducibility is still not completely achieved by the test suite, but using it I have identified a number of very problematic functions, on which the optimizer fails pretty consistently. In fact, on 11 objective functions, there are 5 on which the failure rate is >95%. (Note: this is not just 1 - success_rate; the failure rate is really the ratio of optimizer runs that *failed* abnormally.) I suspect this is due to the fix I introduced earlier which causes the optimizer to noisily die if it does not have sufficient minima to proceed. Furthermore, these functions seem to be the kind that the optimizer would solve immediately, by starting by chance in the neighbourhood of the global minimum. Indeed, the cigar function and sphere function are such functions, and their failure rates are 100%. The idea is this: the optimizer picks two starting points, both lead to the same minimum (the global minimum, probably), and then the optimizer dies because it requires that there be at least two distinct minima to proceed with the optimization. As for the functions with high but non-100% failure rates, I suspect that their high failure rate arises because they have few local minima. Therefore, the probability of the optimizer picking distinct minima is low, but nonzero, which explains the failure rates of 98% and 96% for the h1 function and rosenbrock functions, respectively. From the graphs of these function in 3D, I can see that the neighbourhood of the global minimum is very large.
The fix that I propose for this is rather than to report a failure status when the optimizer encounters such a case, it can instead exit "successfully", simply reporting the value that it has found immediately. Of course, its exit message in that case will be something along the lines of "Local minima exhausted," and its exit code will be nonzero, such that any statistics-collecting software can appropriately report such cases.
Another important thing to add to the test suite would be multiprocessing. This would allow the testing to proceed about four times faster by exploiting multiple cores of the processor, one for each objective function. Race conditions should not occur because the processes would share no data, and would never access the same files.
