07-14
=====
Writing the test sequence for the optimizer. This will pipeline testing.
The idea is to just have to run this nice ``conduct_experiment'' function that will simply collect all the relevant statistics about a run of 100 optimizations (or some other configurable number) on many different objective functions. Such an experiment might take up to 10 minutes to run, and we need to ensure a way to reproduce these experiments reliably. It seems to me that the simplest way of doing so would be to clone the source code of the optimizer (dm_optimizer.py) into the experiment folder, as well as provide a way to dump the configuration of the optimizer. Doing so, it will become possible to definitely reproduce any past experiment.

07-15
=====
Reproducibility is still not completely achieved by the test suite, but using it I have identified a number of very problematic functions, on which the optimizer fails pretty consistently. In fact, on 11 objective functions, there are 5 on which the failure rate is >95%. (Note: this is not just 1 - success_rate; the failure rate is really the ratio of optimizer runs that *failed* abnormally.) I suspect this is due to the fix I introduced earlier which causes the optimizer to noisily die if it does not have sufficient minima to proceed. Furthermore, these functions seem to be the kind that the optimizer would solve immediately, by starting by chance in the neighbourhood of the global minimum. Indeed, the cigar function and sphere function are such functions, and their failure rates are 100%. The idea is this: the optimizer picks two starting points, both lead to the same minimum (the global minimum, probably), and then the optimizer dies because it requires that there be at least two distinct minima to proceed with the optimization. As for the functions with high but non-100% failure rates, I suspect that their high failure rate arises because they have few local minima. Therefore, the probability of the optimizer picking distinct minima is low, but nonzero, which explains the failure rates of 98% and 96% for the h1 function and rosenbrock functions, respectively. From the graphs of these function in 3D, I can see that the neighbourhood of the global minimum is very large.
The fix that I propose for this is rather than to report a failure status when the optimizer encounters such a case, it can instead exit "successfully", simply reporting the value that it has found immediately. Of course, its exit message in that case will be something along the lines of "Local minima exhausted," and its exit code will be nonzero, such that any statistics-collecting software can appropriately report such cases.
Another important thing to add to the test suite would be multiprocessing. This would allow the testing to proceed about four times faster by exploiting multiple cores of the processor, one for each objective function. Race conditions should not occur because the processes would share no data, and would never access the same files.

Introducing the fix proposed above -- changing the exhaustion of local minima into a success condition -- did not reduce the failure rates of the functions mentioned above. This doesn't actually make sense, since failure is measured by counting the runs whose success attribute is False.

Examining the exit messages of the runs proved insightful, and showed that failures are originating from many other places. I managed to suppress some of the failures originating from an exception that used to be thrown when the deltay_curr was too small (instead, I set it to a kind of pseudo).

I tried implemented mutliprocessing to increase the speed, by running 4 optimizations simultaneously. However, this requires passing functions to the child processes (namely the objective functions), and that's done with pickle, which can't deal with functions. Since I can just load the objective functions from DEAP though, it might be worthwhile to just pass the name of the function, and from that I can load it dynamically (yay for interpreted languages, for once). This will greatly increase the speed of the testing.

Tomorrow I need to track down the other sources of failures by examining the exit messages of the tests, and I need to fix multiprocessing. I also need to generalize / clone ``conduct_all_experiments'' to work on simulated annealing as well, that way I can compare them. As for fair comparison, I should try to look into something like scaling the time taken by the success rate, or something.

07-16
=====
Failures fixed. Simply swallowing the exeptions and returning the best value so far results in improved success. I feel that simply swallowing exceptions and bailing out is pretty shitty though. It would be nice if there were some clever way of continuing / avoiding these situations in the first place. Of course, I suspect that these situations arise as a result of the deletion technique. What happens to performance if we play with the refresh rate ?
