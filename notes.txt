07-14
=====
Writing the test sequence for the optimizer. This will pipeline testing.
The idea is to just have to run this nice ``conduct_experiment'' function that will simply collect all the relevant statistics about a run of 100 optimizations (or some other configurable number) on many different objective functions. Such an experiment might take up to 10 minutes to run, and we need to ensure a way to reproduce these experiments reliably. It seems to me that the simplest way of doing so would be to clone the source code of the optimizer (dm_optimizer.py) into the experiment folder, as well as provide a way to dump the configuration of the optimizer. Doing so, it will become possible to definitely reproduce any past experiment.

07-15
=====
Reproducibility is still not completely achieved by the test suite, but using it I have identified a number of very problematic functions, on which the optimizer fails pretty consistently. In fact, on 11 objective functions, there are 5 on which the failure rate is >95%. (Note: this is not just 1 - success_rate; the failure rate is really the ratio of optimizer runs that *failed* abnormally.) I suspect this is due to the fix I introduced earlier which causes the optimizer to noisily die if it does not have sufficient minima to proceed. Furthermore, these functions seem to be the kind that the optimizer would solve immediately, by starting by chance in the neighbourhood of the global minimum. Indeed, the cigar function and sphere function are such functions, and their failure rates are 100%. The idea is this: the optimizer picks two starting points, both lead to the same minimum (the global minimum, probably), and then the optimizer dies because it requires that there be at least two distinct minima to proceed with the optimization. As for the functions with high but non-100% failure rates, I suspect that their high failure rate arises because they have few local minima. Therefore, the probability of the optimizer picking distinct minima is low, but nonzero, which explains the failure rates of 98% and 96% for the h1 function and rosenbrock functions, respectively. From the graphs of these function in 3D, I can see that the neighbourhood of the global minimum is very large.
The fix that I propose for this is rather than to report a failure status when the optimizer encounters such a case, it can instead exit "successfully", simply reporting the value that it has found immediately. Of course, its exit message in that case will be something along the lines of "Local minima exhausted," and its exit code will be nonzero, such that any statistics-collecting software can appropriately report such cases.
Another important thing to add to the test suite would be multiprocessing. This would allow the testing to proceed about four times faster by exploiting multiple cores of the processor, one for each objective function. Race conditions should not occur because the processes would share no data, and would never access the same files.

Introducing the fix proposed above -- changing the exhaustion of local minima into a success condition -- did not reduce the failure rates of the functions mentioned above. This doesn't actually make sense, since failure is measured by counting the runs whose success attribute is False.

Examining the exit messages of the runs proved insightful, and showed that failures are originating from many other places. I managed to suppress some of the failures originating from an exception that used to be thrown when the deltay_curr was too small (instead, I set it to a kind of pseudo).

I tried implemented mutliprocessing to increase the speed, by running 4 optimizations simultaneously. However, this requires passing functions to the child processes (namely the objective functions), and that's done with pickle, which can't deal with functions. Since I can just load the objective functions from DEAP though, it might be worthwhile to just pass the name of the function, and from that I can load it dynamically (yay for interpreted languages, for once). This will greatly increase the speed of the testing.

Tomorrow I need to track down the other sources of failures by examining the exit messages of the tests, and I need to fix multiprocessing. I also need to generalize / clone ``conduct_all_experiments'' to work on simulated annealing as well, that way I can compare them. As for fair comparison, I should try to look into something like scaling the time taken by the success rate, or something.

07-16
=====
Failures fixed. Simply swallowing the exeptions and returning the best value so far results in improved success. I feel that simply swallowing exceptions and bailing out is pretty shitty though. It would be nice if there were some clever way of continuing / avoiding these situations in the first place. Of course, I suspect that these situations arise as a result of the deletion technique. What happens to performance if we play with the refresh rate ?

Now that I think of it, actually, here, I'm checking for success only by looking at the function value. Let's suppose that there is a local minimum whose function value is below the threshold but not truly equal to the optimal value. In that case, I'm counting this run as a success, despite it not being *at the right place* (on the x-axis). I could include a check on the x-coordinate as well, but some objective functions have funny x-coordinates that depend on the dimensions. It would require restructuring a decent chunk of the test framework... I think that in practice, it's *okay*. So what, we don't find the true global minimum, but we have instead some value that's right next to it and whose score on the objective function is still pretty goddamn low. Practically speaking, such a point is pretty good for whatever purpose we want.

Performance for a refresh rate of 8 decreases overall to 55.7%. It seems that Ackley suffered the most. The Simon function #2 (which is atrociously slow), has an extremely low success rate of 0.8%, and is also the function with the highest average runtime. Increasing the refresh rate to 20 brings the global success rate down to 54%. This seems to just be the effect of Ackley's performance dropping. The other functions don't change in a statistically significant way, so it seems.

07-17
=====
Since we want to extend this version control scheme that I have crafted to guarantee reproducibility, I need to generalize my run_scripts.py. First, I'll add a separate module that it loads that contains the list of files to watch, ``reproducible.py''. Then, I'll rename run_tests.py to run_reproducible.py. Simply, load a module specified on the command line, and execute that module's conduct_all_experiments or run function. That function must return a directory where run_reproducible can store the hash of the commit hash.

At the end of the day, everything seems in place. run_reproducible has its own github repository now https://github.com/djeik/reproducible. I didn't get much work done on the optimizing the optimizer proper (heh). On the other hand, some fixes I seem to have introduced a while back have increased the runtime (by a LOT). This is probably due to generalizing the code to allow for SA. This involves a lot of dictionaries and lambdas, which AIUI reaaaaaaally slow. Thanks to my amazing accounting powers and experimental data, I can lookup exactly where this happened and figure out how to fix it. That, however, is for tomorrow.

Holy shit, I wasn't kidding when I said really slow. IT'S INSANELY SLOW. Around 23 minutes for DM!!! It's certainly the dicts and lambdas, but still. That's madness. It was taking around 250 seconds before. This is a ~5x increase! Tomorrow will be a day of much profiling.

07-18
=====
No real work on the optimizer today, but discussed with Simon different approaches for improving it:
* Look into cooling schedules: it's something we'd like to avoid, but if it turns out to really make DM shine, then it might be worthwhile.
* Use more than one target value: by maintaining several target values, we can try to make several steps and check each one locally. By examining each one and using a configurable decision procedure, we can get a more holistic view of the search space and try to make smarter jumps.
* Look into those unrecoverable scenarios, and try to make them recoverable. This might only be fixable by playing with cooling schedules, and our procedure for dropping past minima.

07-23
=====
No real work on the optimizer today, but more discussion of future directions with Simon.
* use SA as a benchmark to determine what's the best dimension for testing each objective function
* try playing with the number of iterations in SA: do higher counts necessarily improve it? (For DM, playing with the counts is basically unnecessary past some threshold around 250, it seems.)
* try fancy "sniffing before digging" methods, like line searches and checking what happens if we step in many different directions, based on more than one past local minimum.
Really weird bug... seems like only three iterations are recorded out of the 250 ! Is seems that this bug was introduced when I switched to the generalized system in which I can test DM and SA uniformly...

07-24
=====
Tracked down that weird bug. It meant writing out a lot of for loops and bashing my head on the wall, but it seems to work now. 
There's something weird going on: the optimizer fails pretty frequently, but the averages are calculated only on the number of runs that produced some output.... so we get 100% success on the sphere function, but in reality, only 86/250 runs actually did something ! I think I need to relax those conditions on the starting positions, or do something in any case so that every run at least does something. That's one of the hypothetical features of our method: it is capable of continuing to search essentially forever, but as it is currently, it just dies (pretty often!)

07-25
=====
Wrote find_test_dimension.py to look for the best dimension for testing an objective function in. The idea is to use SA as a benchmark: run SA on the objective function but keep boosting the dimension number until SA's success rate is below 20%; the dimension at which this occurs is the best dimension to test the function at. This will be especially useful for those functions that we seem to get 100% success rate on; perhaps at some higher dimension, they do really badly, so it would be worthwhile to figure out when that occurs, if it does.
Yeah no, it turns out that there are three functions that SA can solve in arbitrarily high dimensions. Perhaps they aren't worth testing? I suspect that's because finding their global minimum is the only local minimum or something like that.

Think about the "minimum density" d(x), i.e. the average distance to a minimum at a given point x. First, is it useful? Perhaps it can be used to drive "exploration" but pushing away from places where many local minima have been discovered? Perhaps it can be used to determine the maximum step length? The obvious disadvantage of that, I suspect, is that the average step length will increase over time, whereas we would prefer that it vary in both directions. Perhaps, we can check the centroids of groups of three local minima? If we do a local search at the center and fall on one of the minima we already know, maybe we can use this to mark off the interior of that region as not useful.

07-28
=====
Hunting down the slowness....

Still fast:
 * 2014-07-16 13:34:09.105242
 * 2014-07-16 14:58:04.701067
 * 2014-07-16 15:29:08.668288
 * 2014-07-16 15:50:02.031571 (no failures) -- 5fb758e0f7d85fdf1bfc1c5a3e854449399d025e (CLEAN)
 ^ that's the last one where things are fast. It turns out that we even have good success on functions that we currently don't !

dm 2014-07-16 17:13:13.790325     --  5fb758e0f7d85fdf1bfc1c5a3e854449399d025e (NOT CLEAN, fuck...)
Another one later that is also slow: a53e3102d73e7e21717099283ebdbc6fe9fb1ccd

Note: I've started a branch at the last fast commit called 'last-fast'.

Starting with this output, everything is SO SLOW.
This seems to be where I introduced the generalization to allow for SA.
Yes, from the commit whose message is "add SA and generalize testing code" everything becomes very very slow. The average number of function evaluations jumps to 45000, whereas before it was 15000. Furthermore, the success rate of the rastrigin function drops from ~60% to zero! 

From some rudimentary sniffing around, it looks like the code that handles maximization is what's causing the slowdown, as it seems that it was after its introduction that performance began to suffer (2-second average runtimes for some functions!)

Okay no. It's not that. The maximization does introduce an extra layer of function calls, but it's not what's causing the slowdown. It seems that the average number of function evaluations has increased between the "fast" runs and "slow" runs, so really, what's happening is that the slow runs are evaluating the objective function far more frequently, which is what's resulting in the slowdown. In fact, from some back-of-the-envelope math, it seems that the rosenbrock function (which can take up to 10 seconds to do one run!) is being evaluated 11 BILLION times in total ! Simply *evaluating a function* (ANY FUNCTION!) that many times takes roughly 800 seconds. (Hooray, python...) My conclusion is thus that it's not the *testing* code that is at fault, but rather the actual optimizer... I must have changed something that is causing the objective function to be evaluated more frequently. 

It doesn't make sense that the number of function evaluations should increase so much though... Nothing changed in dm_optimizer.py save for some two lines relating to bookkeeping, so the extra evals are coming from changes in the testing suite, which DOESN'T MAKE SENSE >.<

Could it be the refresh rate? The refresh rate is controlled by the testing suite, in the configuration. That might also explain the drop in performance in the rastrigin function, among others. I am not quite sure how that explains the function evaluations, however...

That doesn't seem to have resolved anything at all... It still takes some 30 minutes for DM to complete. Another way of verifying the slowness would be to check whether SA has gotten slower since that commit when things used to be fast. The thing is that we can't really use SA there because we haven't yet generalized the code for it to work! Everything is DM specific in that commit. It doesn't make sense however that generalizing only a small amount like I did could have such a negative impact on performance.

07-29
=====
IDEA: could it be that we split up the configuration for DM and SA that the max_iterations are not being properly passed to dm_optimizer ? Since we use the max_iterations correctly in the output code, it would make sense that the total number of output info is properly limited to 250 or 150 (or some other small number) but DM is using its default value of... 2500! That would explain also the differences I see when inspecting the optimizer exit messages: in the ultra-slow ones, I see mostly "All local minima have converged to a point. Optimization cannot proceed," whereas in the fast ones, I see mostly "Maximum number of iterations reached." How does this explain the poorer performance in the slow one? Well... It doesn't really. So I'm confused about that one. 

Damn. It seems that the correct kwargs are being passed to dm_optimizer. It's really only running the right number of times.

It turns out that... this was all expected behaviour. It turns out that in that commit in which I generalize the code to work for SA that I change the default number of dimensions tested from 2 to 5, and that some of the objective functions, especially the Rosenbrock function, take superlinear time in the number of dimensions. (Rosenbrock appears quadratic from what I saw in its source code.) That explains why Rosenbrock goes from 200 seconds to... a large number.

So now there's another bug woooooooo!! Looks like a divide by zero is occurring based on self.iteration in dm_tests, causing the subprocesses to die. Without knowing it, commit dff4f0c8d4cf35065de1a43104cebd6245bf1abc was mitigating this, since self.iteration becomes an ordinal, and so starts at 1. Therefore, the nit stored in the OptimizeResult is 1 and the divide by zero does not occur.

07-30
=====
Well I'm silly. The reason, it seems, why it was failing with a divide by zero is simply that no data was coming from the inner optimizer... because I was writing xrange(stop, start) but it should be xrange(start, stop). Of course, when you give xrange with only one argument, it's xrange(stop) and the start is implied to be zero. But if you want to give both you're in for a surprise because the stop moves over... Anyway...

Seems like it was the transposing that was failing. Seems like it's fixed now, since I've written the transpose out by hand. 

