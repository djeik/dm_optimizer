07-14
=====
Writing the test sequence for the optimizer. This will pipeline testing.
The idea is to just have to run this nice ``conduct_experiment'' function that will simply collect all the relevant statistics about a run of 100 optimizations (or some other configurable number) on many different objective functions. Such an experiment might take up to 10 minutes to run, and we need to ensure a way to reproduce these experiments reliably. It seems to me that the simplest way of doing so would be to clone the source code of the optimizer (dm_optimizer.py) into the experiment folder, as well as provide a way to dump the configuration of the optimizer. Doing so, it will become possible to definitely reproduce any past experiment.

07-15
=====
Reproducibility is still not completely achieved by the test suite, but using it I have identified a number of very problematic functions, on which the optimizer fails pretty consistently. In fact, on 11 objective functions, there are 5 on which the failure rate is >95%. (Note: this is not just 1 - success_rate; the failure rate is really the ratio of optimizer runs that *failed* abnormally.) I suspect this is due to the fix I introduced earlier which causes the optimizer to noisily die if it does not have sufficient minima to proceed. Furthermore, these functions seem to be the kind that the optimizer would solve immediately, by starting by chance in the neighbourhood of the global minimum. Indeed, the cigar function and sphere function are such functions, and their failure rates are 100%. The idea is this: the optimizer picks two starting points, both lead to the same minimum (the global minimum, probably), and then the optimizer dies because it requires that there be at least two distinct minima to proceed with the optimization. As for the functions with high but non-100% failure rates, I suspect that their high failure rate arises because they have few local minima. Therefore, the probability of the optimizer picking distinct minima is low, but nonzero, which explains the failure rates of 98% and 96% for the h1 function and rosenbrock functions, respectively. From the graphs of these function in 3D, I can see that the neighbourhood of the global minimum is very large.
The fix that I propose for this is rather than to report a failure status when the optimizer encounters such a case, it can instead exit "successfully", simply reporting the value that it has found immediately. Of course, its exit message in that case will be something along the lines of "Local minima exhausted," and its exit code will be nonzero, such that any statistics-collecting software can appropriately report such cases.
Another important thing to add to the test suite would be multiprocessing. This would allow the testing to proceed about four times faster by exploiting multiple cores of the processor, one for each objective function. Race conditions should not occur because the processes would share no data, and would never access the same files.

Introducing the fix proposed above -- changing the exhaustion of local minima into a success condition -- did not reduce the failure rates of the functions mentioned above. This doesn't actually make sense, since failure is measured by counting the runs whose success attribute is False.

Examining the exit messages of the runs proved insightful, and showed that failures are originating from many other places. I managed to suppress some of the failures originating from an exception that used to be thrown when the deltay_curr was too small (instead, I set it to a kind of pseudo).

I tried implemented mutliprocessing to increase the speed, by running 4 optimizations simultaneously. However, this requires passing functions to the child processes (namely the objective functions), and that's done with pickle, which can't deal with functions. Since I can just load the objective functions from DEAP though, it might be worthwhile to just pass the name of the function, and from that I can load it dynamically (yay for interpreted languages, for once). This will greatly increase the speed of the testing.

Tomorrow I need to track down the other sources of failures by examining the exit messages of the tests, and I need to fix multiprocessing. I also need to generalize / clone ``conduct_all_experiments'' to work on simulated annealing as well, that way I can compare them. As for fair comparison, I should try to look into something like scaling the time taken by the success rate, or something.

07-16
=====
Failures fixed. Simply swallowing the exeptions and returning the best value so far results in improved success. I feel that simply swallowing exceptions and bailing out is pretty shitty though. It would be nice if there were some clever way of continuing / avoiding these situations in the first place. Of course, I suspect that these situations arise as a result of the deletion technique. What happens to performance if we play with the refresh rate ?

Now that I think of it, actually, here, I'm checking for success only by looking at the function value. Let's suppose that there is a local minimum whose function value is below the threshold but not truly equal to the optimal value. In that case, I'm counting this run as a success, despite it not being *at the right place* (on the x-axis). I could include a check on the x-coordinate as well, but some objective functions have funny x-coordinates that depend on the dimensions. It would require restructuring a decent chunk of the test framework... I think that in practice, it's *okay*. So what, we don't find the true global minimum, but we have instead some value that's right next to it and whose score on the objective function is still pretty goddamn low. Practically speaking, such a point is pretty good for whatever purpose we want.

Performance for a refresh rate of 8 decreases overall to 55.7%. It seems that Ackley suffered the most. The Simon function #2 (which is atrociously slow), has an extremely low success rate of 0.8%, and is also the function with the highest average runtime. Increasing the refresh rate to 20 brings the global success rate down to 54%. This seems to just be the effect of Ackley's performance dropping. The other functions don't change in a statistically significant way, so it seems.

07-17
=====
Since we want to extend this version control scheme that I have crafted to guarantee reproducibility, I need to generalize my run_scripts.py. First, I'll add a separate module that it loads that contains the list of files to watch, ``reproducible.py''. Then, I'll rename run_tests.py to run_reproducible.py. Simply, load a module specified on the command line, and execute that module's conduct_all_experiments or run function. That function must return a directory where run_reproducible can store the hash of the commit hash.

At the end of the day, everything seems in place. run_reproducible has its own github repository now https://github.com/djeik/reproducible. I didn't get much work done on the optimizing the optimizer proper (heh). On the other hand, some fixes I seem to have introduced a while back have increased the runtime (by a LOT). This is probably due to generalizing the code to allow for SA. This involves a lot of dictionaries and lambdas, which AIUI reaaaaaaally slow. Thanks to my amazing accounting powers and experimental data, I can lookup exactly where this happened and figure out how to fix it. That, however, is for tomorrow.

Holy shit, I wasn't kidding when I said really slow. IT'S INSANELY SLOW. Around 23 minutes for DM!!! It's certainly the dicts and lambdas, but still. That's madness. It was taking around 250 seconds before. This is a ~5x increase! Tomorrow will be a day of much profiling.

07-18
=====
No real work on the optimizer today, but discussed with Simon different approaches for improving it:
* Look into cooling schedules: it's something we'd like to avoid, but if it turns out to really make DM shine, then it might be worthwhile.
* Use more than one target value: by maintaining several target values, we can try to make several steps and check each one locally. By examining each one and using a configurable decision procedure, we can get a more holistic view of the search space and try to make smarter jumps.
* Look into those unrecoverable scenarios, and try to make them recoverable. This might only be fixable by playing with cooling schedules, and our procedure for dropping past minima.

07-23
=====
No real work on the optimizer today, but more discussion of future directions with Simon.
* use SA as a benchmark to determine what's the best dimension for testing each objective function     (done)
* try playing with the number of iterations in SA: do higher counts necessarily improve it? (For DM, playing with the counts is basically unnecessary past some threshold around 250, it seems.)
* try fancy "sniffing before digging" methods, like line searches and checking what happens if we step in many different directions, based on more than one past local minimum.
Really weird bug... seems like only three iterations are recorded out of the 250 ! Is seems that this bug was introduced when I switched to the generalized system in which I can test DM and SA uniformly...

07-24
=====
Tracked down that weird bug. It meant writing out a lot of for loops and bashing my head on the wall, but it seems to work now. 
There's something weird going on: the optimizer fails pretty frequently, but the averages are calculated only on the number of runs that produced some output.... so we get 100% success on the sphere function, but in reality, only 86/250 runs actually did something ! I think I need to relax those conditions on the starting positions, or do something in any case so that every run at least does something. That's one of the hypothetical features of our method: it is capable of continuing to search essentially forever, but as it is currently, it just dies (pretty often!)

07-25
=====
Wrote find_test_dimension.py to look for the best dimension for testing an objective function in. The idea is to use SA as a benchmark: run SA on the objective function but keep boosting the dimension number until SA's success rate is below 20%; the dimension at which this occurs is the best dimension to test the function at. This will be especially useful for those functions that we seem to get 100% success rate on; perhaps at some higher dimension, they do really badly, so it would be worthwhile to figure out when that occurs, if it does.
Yeah no, it turns out that there are three functions that SA can solve in arbitrarily high dimensions. Perhaps they aren't worth testing? I suspect that's because finding their global minimum is the only local minimum or something like that.

Think about the "minimum density" d(x), i.e. the average distance to a minimum at a given point x. First, is it useful? Perhaps it can be used to drive "exploration" but pushing away from places where many local minima have been discovered? Perhaps it can be used to determine the maximum step length? The obvious disadvantage of that, I suspect, is that the average step length will increase over time, whereas we would prefer that it vary in both directions. Perhaps, we can check the centroids of groups of three local minima? If we do a local search at the center and fall on one of the minima we already know, maybe we can use this to mark off the interior of that region as not useful.

07-28
=====
Hunting down the slowness....

Still fast:
 * 2014-07-16 13:34:09.105242
 * 2014-07-16 14:58:04.701067
 * 2014-07-16 15:29:08.668288
 * 2014-07-16 15:50:02.031571 (no failures) -- 5fb758e0f7d85fdf1bfc1c5a3e854449399d025e (CLEAN)
 ^ that's the last one where things are fast. It turns out that we even have good success on functions that we currently don't !

dm 2014-07-16 17:13:13.790325     --  5fb758e0f7d85fdf1bfc1c5a3e854449399d025e (NOT CLEAN, fuck...)
Another one later that is also slow: a53e3102d73e7e21717099283ebdbc6fe9fb1ccd

Note: I've started a branch at the last fast commit called 'last-fast'.

Starting with this output, everything is SO SLOW.
This seems to be where I introduced the generalization to allow for SA.
Yes, from the commit whose message is "add SA and generalize testing code" everything becomes very very slow. The average number of function evaluations jumps to 45000, whereas before it was 15000. Furthermore, the success rate of the rastrigin function drops from ~60% to zero! 

From some rudimentary sniffing around, it looks like the code that handles maximization is what's causing the slowdown, as it seems that it was after its introduction that performance began to suffer (2-second average runtimes for some functions!)

Okay no. It's not that. The maximization does introduce an extra layer of function calls, but it's not what's causing the slowdown. It seems that the average number of function evaluations has increased between the "fast" runs and "slow" runs, so really, what's happening is that the slow runs are evaluating the objective function far more frequently, which is what's resulting in the slowdown. In fact, from some back-of-the-envelope math, it seems that the rosenbrock function (which can take up to 10 seconds to do one run!) is being evaluated 11 BILLION times in total ! Simply *evaluating a function* (ANY FUNCTION!) that many times takes roughly 800 seconds. (Hooray, python...) My conclusion is thus that it's not the *testing* code that is at fault, but rather the actual optimizer... I must have changed something that is causing the objective function to be evaluated more frequently. 

It doesn't make sense that the number of function evaluations should increase so much though... Nothing changed in dm_optimizer.py save for some two lines relating to bookkeeping, so the extra evals are coming from changes in the testing suite, which DOESN'T MAKE SENSE >.<

Could it be the refresh rate? The refresh rate is controlled by the testing suite, in the configuration. That might also explain the drop in performance in the rastrigin function, among others. I am not quite sure how that explains the function evaluations, however...

That doesn't seem to have resolved anything at all... It still takes some 30 minutes for DM to complete. Another way of verifying the slowness would be to check whether SA has gotten slower since that commit when things used to be fast. The thing is that we can't really use SA there because we haven't yet generalized the code for it to work! Everything is DM specific in that commit. It doesn't make sense however that generalizing only a small amount like I did could have such a negative impact on performance.

07-29
=====
IDEA: could it be that we split up the configuration for DM and SA that the max_iterations are not being properly passed to dm_optimizer ? Since we use the max_iterations correctly in the output code, it would make sense that the total number of output info is properly limited to 250 or 150 (or some other small number) but DM is using its default value of... 2500! That would explain also the differences I see when inspecting the optimizer exit messages: in the ultra-slow ones, I see mostly "All local minima have converged to a point. Optimization cannot proceed," whereas in the fast ones, I see mostly "Maximum number of iterations reached." How does this explain the poorer performance in the slow one? Well... It doesn't really. So I'm confused about that one. 

Damn. It seems that the correct kwargs are being passed to dm_optimizer. It's really only running the right number of times.

It turns out that... this was all expected behaviour. It turns out that in that commit in which I generalize the code to work for SA that I change the default number of dimensions tested from 2 to 5, and that some of the objective functions, especially the Rosenbrock function, take superlinear time in the number of dimensions. (Rosenbrock appears quadratic from what I saw in its source code.) That explains why Rosenbrock goes from 200 seconds to... a large number.

So now there's another bug woooooooo!! Looks like a divide by zero is occurring based on self.iteration in dm_tests, causing the subprocesses to die. Without knowing it, commit dff4f0c8d4cf35065de1a43104cebd6245bf1abc was mitigating this, since self.iteration becomes an ordinal, and so starts at 1. Therefore, the nit stored in the OptimizeResult is 1 and the divide by zero does not occur.

07-30
=====
Well I'm silly. The reason, it seems, why it was failing with a divide by zero is simply that no data was coming from the inner optimizer... because I was writing xrange(stop, start) but it should be xrange(start, stop). Of course, when you give xrange with only one argument, it's xrange(stop) and the start is implied to be zero. But if you want to give both you're in for a surprise because the stop moves over... Anyway...

Seems like it was the transposing that was failing. Seems like it's fixed now, since I've written the transpose out by hand. 

Turns out that the cigar function fails immediately. It doesn't even make it through one iteration (the callback is never called.)

Idea: try to step toward each of the past minima, using the same scaling method (i.e. the difference between that minimum's y-value and the target is used to scale) and pick the direction that yields the lowest value.
This would mean generalizing/parameterizing the scaling function, which could be useful in the long run as it gives me a centralized place to work in when it comes to the scaling factors.

It seems that using this method of looking at all the past minima doesn't really change anything. The success rates haven't budged in using it. On the other hand, I think that coming up with a more satisfactory algorithm for calculating the step scale based on .... well, I'm not sure yet. Right now it's based on the ratio of the distances to the target. IT TURNS OUT THAT if you make it a constant 50%... there's a really nice improvement. Heh.

Talked with Simon, some thoughts:
 * Compare with other solvers, not just SA, especially not just SA in python. Mathematica would be a good thing to try. Simon said that he might do that since he's more familiar with Mathematica. I would also need to get Mathematica installed. I really should get IT to do that for me...
 * do an *actual* comparison with SA. As in, find a way to actually put a number on the difference in performance.
 * find a less sketchy way of scoring the performance of DM and SA, perhaps by taking a ratio of success and function evaluations. Simon said in practice, the number of function evaluations is important because the functions are tough to evaluate, but are otherwise not terrifically difficult to solve. Hm.
 * do a truly directional 1/2-search. So right now we have this ultra-greedy search using a step that's 1/2 the distance between the current minimum and the best minimum. It could be that what I have already has this, but I'm not sure. The idea is that it needs to *keep moving in the direction of the best minimum*. So, suppose that I'm at some minimum A that is AFAIK the global minimum. I want to do another iteration so I find the best minimum that is not the one that I'm at and call it B. Rather than step *toward B*, I should step *from B to A*, since A is better than B! I don't think that's what I'm currently doing, but it's what I should do.
 * Look back into the minimum-dropping technique. Investigate whether I can replicate what it's doing without doing it :P
 * Line search: evaluate the objective function N times along the *line* defined by that step vector. Take the minimal place as the new location of the iterate.
 * Try different constants for the 1/2 method. In that case it wouldn't be a 1/2 method, now would it.
 * See whether bracketing is actually what's causing the significant success increase in the Griewank function.

08-08
=====
So I've implemented a kind of performance measure based on the number of function evaluations per success; smaller values are better. In the cases where the number of successes is zero, then I simply don't count that run towards the average performance. 

Plotting is still broken. Let's look at some sample output from Ackley: (function_value.txt)
18.5974318431   19.2283354885  17.7830204938  17.8118686259  17.8466315343  17.7830204938  None           None           None           None           None           None
18.2216926693   17.8118686304  15.2236423435  13.9242956146  14.1008149469  13.8786458041  14.143491012   14.2271837228  13.8786458041  None           None           None
17.7856391945   18.5853429536  19.7852007045  19.0040737652  18.7132880968  18.5841552596  18.5853429535  18.5841552596  None           None           None           None
17.5676914986   19.0683152098  19.7999241674  19.3533977423  19.1165601866  19.0683152098  None           None           None           None           None           None
17.4246455908   19.2335809224  19.8912558203  19.48623378    19.3169499344  19.2335809224  None           None           None           None           None           None
13.1095090492   18.6351435018  17.8637134896  17.7282887987  17.7919554376  17.7282887987  None           None           None           None           None           None
8.3539503274    17.3306432991  19.6693780906  18.2836152038  17.5824995569  17.4447187076  17.3306432991  None           None           None           None           None
4.40495239475   19.5250988418  19.8246293632  19.6068527256  19.5623535036  19.5250988418  None           None           None           None           None           None
2.87991069167   17.8594614998  19.7080320685  18.5661737461  17.9819027109  17.8184484388  17.9542533248  17.7762848346  17.8184484388  17.7762848346  None           None
1.89917857862   19.6410972591  19.4230789425  19.3612910302  19.374707388   19.4168895541  19.3254222545  19.2906906034  19.2714755835  19.2714755835  None           None
1.52860013572   19.0882843029  19.9625894819  19.5567096651  19.2041410677  19.1006390203  19.0882843029  None           None           None           None           None
1.14729881615   17.8162583527  18.5947953008  17.9819026986  17.8509205633  17.8162583527  None           None           None           None           None           None
0.93645800324   19.3686545498  18.4884061081  18.0617061376  18.1595599413  18.0561570939  18.0487229837  18.087304904   18.0487229837  None           None           None
0.936104293508  19.6101704101  19.9632574556  19.7629386187  19.6460144398  19.6101704101  None           None           None           None           None           None
0.935985697107  18.1388196991  13.9581243418  14.4848973376  13.6281766403  13.7137544063  13.6281766403  None           None           None           None           None

The first thing to notice is that None value phase in and out. That shouldn't be possible. Once None occurs in a column, all subsequent values in that column should be None: it represents that that run prematurely terminated. This makes me thing that I'm padding the lists at the wrong time, before or after some transpose. This is probably why the plots look weird.

Yay, that's fixed. In fact, the transpose was pretty much broken.

Talked with Simon:
 * My priority should be on getting some really nice graphics for comparing DM and SA. He suggests graphs showing the fraction of solved runs as a function of the iteration number (see paper in folder).
 * I should look back on the past strategies, esp. the minimum-dropping method, and see with this comparison whether it really beats SA.
 * I should see with some other constants for the fixed-step method, e.g. 0.1 and 1.0. 
 * I need to write the summary of the algorithm.

I should probably use a very high number of iterations for that fraction solved vs time graph, around 4000. This means that running it on abacus is probably the best way to do things. I'll need to adapt the code in run_reproducible_pipeline then, since as of right now, it fails with a file not found error about git...

08-13
=====
Got some nice nice graphs! Simon really likes them too. I should launch a bunch of jobs on abacus parameterizing over the step scaling factor, to see its influence. I should also raise the number of dimensions in some the objective functions. I'm running many of them in just 2 (or even 1!) dimensions because when I first did my little check to see what dimension to try in, I was using SA with an iteration count that was significantly lower than what I'm currently using (1000 iterations).
Once I have those jobs running, I need to try to see if I get this sort of spiking-up growth in simulated annealing (cf. plots from this morning) even if I use a lower number of iterations. I probably will, and that will indicate to me that simulated annealing's default cooling schedule is some sort of exponential decrease, I guess.
I also need to get Mathematica so that I can compare with other solvers. This also means that I'm going to need to rewrite DM in Mathematica. Isn't that the supreme irony of life? Port the code from mathematica to python, so that when I'm done doing the real developing in Python, I can just port it back to Mathematica.
In the functions where we fail really badly, it would be good to look at the trajectories of the iterate, to see _why_ we fail so badly. That might require moving into lower dimensions temporarily to make some plots of the trajectories.
It would also be worthwhile to see what happens to the plots if we run this for much much longer, e.g. 10000 iterations. 

08-18
=====
Alright, I finally have the nice plots showing the effect of the step scale. It seems in general that smaller values are better, except in some weird cases like the h1 function, where larger values are better! So we seem to be doing equally well, as simulated annealing. 
It would be worthwhile to check out the following:
 * use hypothetical steps in many directions, but from the current local minimum instead of from the iterate;
 * boost the number of dimensions in the easier problems;
 * look into adaptive step scaling heuristics. This is somewhat of a step backwards since the whole idea of using a fixed step scale is to *avoid* an adaptive method there. 

Alright so I have 100+ jobs going on abacus. I feel like a baller :)
It seems that the introduction of the many-step strategy introduced a bug that is causing failures in certain (all?) functions. This needs to be checked out. I've streamlined even more so the collection of data for each function and across many step sizes. I need to change that to do step sizes between 0.1 and 2.0, since now I just go to 1.2. I also need to make it so that when all the jobs complete, the plotting program is called. Furthermore, I need to update the plotting program to make use of the new directory structure that I have created.
