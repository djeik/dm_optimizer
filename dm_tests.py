from __future__ import print_function

import sys
import random
from time import time
from math import sin
from copy import copy

import numpy as np
from numpy.linalg import norm
import deap.benchmarks as bench # various test functions
from scipy.optimize import basinhopping

import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D
from matplotlib import cm

import dm_optimizer as dm
from dm_optimizer import dm_optimizer

# These are the functions Simon defined to test it on:
def simon_f1(xy):
    x, y = xy
    return 0.2 * (x**2 + y**2) / 2 + 10 * sin(x+y)**2 + 10 * sin(100 * (x - y))**2

def simon_f2(xs):
    xy = xs - np.array([100, 100])
    return simon_f1(xy)

# deap's benchmarking functions return the values as tuples with nothing in element 1,
# so we need to unpack element 0 which is the actual function-value.
def unwrap_bench(f):
    return lambda x: f(x)[0]

def random_guess(dim, scale = 1):
    return np.array([scale * (random.uniform(0,1) - 1/2.0) for _ in xrange(dim)])

def test(f, d=2, scale=64, show_plot=True):
    """ Run the dm optimizer on a given function in a given number of dimensions (assuming the given function supports it)
        and possibly show a plot, if the function is in three dimensions.
        """
    optimizer = dm_optimizer(f, max_iterations=500, verbosity=2, greediness=0.2, first_target_ratio=0.75)
    lpos = optimizer.minimize(random_guess(d, 600), random_guess(d, 600))
    fv, xmin = lpos[-1]
    print("Steps taken: ")
    map((lambda p: print(" ", p)), lpos)
    print("Found global minimum f", xmin, " = ", fv, sep='')
    if show_plot and len(xmin) == 2: #only plot if in 3D !
        plotf(f, lpos)

def random_dm(f, d, scale, dm_args={}):
    return dm.minimize(f, random_guess(d, scale), random_guess(d, scale), **dm_args)

def random_sa(f, d, scale, sa_args={}):
    return basinhopping(f, random_guess(d, scale), **sa_args)

def optimizer_test(f, random_opt, d=2, scale=10, n=100, opt_args={}):
    """ Run an optimizer many times, timing the overall execution and collecting the results of each run.

        Arguments:
            f                   -- the objective function.
            random_opt          -- a function that will actually call the optimizer function. See random_dm and random_sa above.
            d                   -- the number of dimensions to evaluate the function in.
            scale               -- the size of the region from which to pick random initial guesses.
            n                   -- the number of times to run the optimizer.
            opt_tolerance       -- the tolerance used internally by the optimizer to test for equality.
            greediness          -- higher values will make the optimizer "aim lower" when performing its target updating step.
            opt_args            -- all remaining keyword arguments are passed to the underlying optimizer.

        Returns:
            A tuple consisting of the list of return values generated by the optimizer and the total time taken to run the optimizer n times.

        Notes:
            For each iteration of the optimization, a new uniform random point is picked as a starting position from the square
            region [-scale/2, scale/2)^2. The target given as an argument to this function is the value the optimizer should arrive at. Although the
            optimizer maintains a target value that it aims for and can operate in a way as to search for that given value, the target given to this
            function is not passed to the optimizer to use internally.
        """
    ress = []

    start_time = time()
    try:
        for i in xrange(n):
            print("Run #", i, sep='')
            res = random_opt(f, d, scale, opt_args)
            ress.append(res)
    except KeyboardInterrupt:
        pass # simply allow aborting.
    end_time = time()

    total_time = end_time - start_time

    return (ress, total_time)

def optimization_stats(results, time, target, success_threshold):
    if len(results) == 0:
        raise ValueError("Results list must be nonempty.")
    t = success_threshold
    n = float(len(results))
    nfev, s, b = 0, 0, 0
    for r in results:
        if (not 'success' in r.keys()) or ('success' in r.keys() and r.success):
            nfev += r.nfev
            s += 1 if (r.fun - target)**2 < t**2 else 0
            b += 1 if norm(r.x) > 1.0e+6         else 0
            print(r.message, file=sys.stderr),
    print("Number of really bad solutions:", b)
    return (s/n, time/n, nfev/n)

def read_2d_csv(filename):
    dats = []
    with open(filename) as f:
        for line in f:
            dats.append(tuple(map(float, line.split(','))))
    return zip(*dats)

def write_2d_csv(fname, dats):
    f = open(fname, 'w')
    for (iter_count, success_rate) in dats:
        print(iter_count, success_rate, sep=',', file=f)
    f.close()

def fig_gen(fname, title):
    plt.clf()
    plt.title(title)
    plt.plot(*read_2d_csv(fname))
    plt.savefig(fname + ".pdf")

# Visual debug tool for 3d
def plotf(f, xyzs_, start=np.array([-1,-1]), end=np.array([1,1]), smoothness=1.0, autobound=True, autosmooth=True):
    fig = plt.figure()
    ax = fig.gca(projection='3d')

    z, xys = zip(*xyzs_) # prepare to draw the line
    x, y = zip(*xys)
    N = len(x)
    for i in xrange(N-1):
        ax.plot(x[i:i+2], y[i:i+2], z[i:i+2], color=plt.cm.jet(255*i/N))

    if autobound:
        start = reduce(np.minimum, xys)
        end = reduce (np.maximum, xys)

    if autosmooth:
        smoothness = norm(end - start) / 100.0

    print(start, end, smoothness)

    xs = np.arange(start[0], end[0], smoothness)
    ys = np.arange(start[1] + y[-1], end[1] + y[-1], smoothness)
    X, Y = np.meshgrid(xs, ys)
    zs = np.array([f((x,y)) for x,y in zip(np.ravel(X), np.ravel(Y))])
    Z = zs.reshape(X.shape)
    surf = ax.plot_surface(X, Y, Z, rstride=1, cstride=1, cmap=cm.coolwarm, # draw the surface
            linewidth=0, antialiased=True)
    fig.colorbar(surf, shrink=0.5, aspect=5)

    plt.show()

def perf_vs_iters(f, algo, d, scale, iters, niter_name='max_iterations', m=100, opt_args={}, target=0.0):
    dats = []
    for i in iters:
        (s, t_avg, nfev_avg) = optimization_stats(*optimizer_test(f, algo, d, scale, m, opt_args={niter_name:i}), target=0.0, success_threshold=0.001)
        dats.append( (i, s, t_avg, nfev_avg) )
        print(dats[-1])
    return dats

def save_plots(dats):
    plt.clf()
    plt.title("Success rate vs iterations (ackley 64, 3D)")
    plt.plot(*zip(*map(lambda (i, s, _1, _2): (i, s), dats)))
    plt.savefig("sa-ackley-64-3d-s-vs-i.pdf")
    plt.clf()
    plt.title("Average function evaluations vs iterations (ackley 64, 3D)")
    plt.plot(*zip(*map(lambda (i, _1, _2, f): (i, f), dats)))
    plt.savefig("sa-ackley-64-3d-f-vs-i.pdf")
    plt.clf()
    plt.title("Average time vs iterations (ackley 64, 3D)")
    plt.plot(*zip(*map(lambda (i, _1, t, _2): (i, t), dats)))
    plt.savefig("sa-ackley-64-3d-t-vs-i.pdf")
