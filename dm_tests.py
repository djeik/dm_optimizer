from __future__ import print_function

import matplotlib
matplotlib.use("TkAgg")

from datetime import datetime
import sys
import random
import os
from time import time
from math import sin
from copy import copy

import numpy as np
from numpy.linalg import norm
import deap.benchmarks as bench # various test functions
from scipy.optimize import basinhopping

import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D
from matplotlib import cm

from itertools import repeat, imap, ifilter, islice, chain

import dm_optimizer as dm
from dm_optimizer import dm_optimizer

# These are the functions Simon defined to test it on:
def simon_f1(xy):
    x, y = xy
    return 0.2 * (x**2 + y**2) / 2 + 10 * sin(x+y)**2 + 10 * sin(100 * (x - y))**2

def simon_f2(xs):
    xy = xs - np.array([100, 100])
    return simon_f1(xy)

# deap's benchmarking functions return the values as tuples with nothing in element 1,
# so we need to unpack element 0 which is the actual function-value.
def unwrap_bench(f):
    return lambda x: f(x)[0]

def random_guess(dim, scale = 1):
    return np.array([scale * (random.uniform(0,1) - 1/2.0) for _ in xrange(dim)])

def randomr_guess(dim, r=(-1,1)):
    return np.array([random.uniform(*rr) for rr in repeat(r, dim)])

def intersperse(delimiter, seq):
        return islice(chain.from_iterable(izip(repeat(delimiter), seq)), 1, None)

def test(f, d=2, scale=64, show_plot=True):
    """ Run the dm optimizer on a given function in a given number of dimensions (assuming the given function supports it)
        and possibly show a plot, if the function is in three dimensions.
        """
    optimizer = dm_optimizer(f, max_iterations=500, verbosity=2, greediness=0.2, first_target_ratio=0.75)
    lpos = optimizer.minimize(random_guess(d, 600), random_guess(d, 600))
    fv, xmin = lpos[-1]
    print("Steps taken: ")
    map((lambda p: print(" ", p)), lpos)
    print("Found global minimum f", xmin, " = ", fv, sep='')
    if show_plot and len(xmin) == 2: #only plot if in 3D !
        plotf(f, lpos)

def random_dm(f, d, scale, dm_args={}):
    return dm.minimize(f, random_guess(d, scale), random_guess(d, scale), **dm_args)

def random_sa(f, d, scale, sa_args={}):
    return basinhopping(f, random_guess(d, scale), **sa_args)

def optimizer_test(f, random_opt, d=2, scale=10, n=100, opt_args={}):
    """ Run an optimizer many times, timing the overall execution and collecting the results of each run.

        Arguments:
            f                   -- the objective function.
            random_opt          -- a function that will actually call the optimizer function. See random_dm and random_sa above.
            d                   -- the number of dimensions to evaluate the function in.
            scale               -- the size of the region from which to pick random initial guesses.
            n                   -- the number of times to run the optimizer.
            opt_tolerance       -- the tolerance used internally by the optimizer to test for equality.
            greediness          -- higher values will make the optimizer "aim lower" when performing its target updating step.
            opt_args            -- all remaining keyword arguments are passed to the underlying optimizer.

        Returns:
            A tuple consisting of the list of return values generated by the optimizer and the total time taken to run the optimizer n times.

        Notes:
            For each iteration of the optimization, a new uniform random point is picked as a starting position from the square
            region [-scale/2, scale/2)^2. The target given as an argument to this function is the value the optimizer should arrive at. Although the
            optimizer maintains a target value that it aims for and can operate in a way as to search for that given value, the target given to this
            function is not passed to the optimizer to use internally.
        """
    ress = []

    start_time = time()
    try:
        for i in xrange(n):
            print("Run #", i, sep='')
            res = random_opt(f, d, scale, opt_args)
            ress.append(res)
    except KeyboardInterrupt:
        pass # simply allow aborting.
    end_time = time()

    total_time = end_time - start_time

    return (ress, total_time)

def optimization_stats(results, time, target, success_threshold):
    if len(results) == 0:
        raise ValueError("Results list must be nonempty.")
    t = success_threshold
    n = float(len(results))
    nfev, s = 0, 0
    for r in results:
        if (not 'success' in r.keys()) or ('success' in r.keys() and r.success):
            nfev += r.nfev
            s += 1 if (r.fun - target)**2 < t**2 else 0
            print(r.message, file=sys.stderr),
    return (s/n, time/n, nfev/n)

def read_2d_csv(filename):
    dats = []
    with open(filename) as f:
        for line in f:
            dats.append(tuple(map(float, line.split(','))))
    return zip(*dats)

def write_2d_csv(fname, dats):
    with open(fname, 'w') as f:
        for (iter_count, success_rate) in dats:
            print(iter_count, success_rate, sep=',', file=f)

def tuples_to_csv(dats):
    return '\n'.join([','.join(map(str, x)) for x in dats])

def csv_to_tuples(csv):
    return [tuple(x.split(',')) for x in csv.split('\n')]

def print_csv(*args, **kwargs):
    print(*args, sep=',', **kwargs)

def write_str(fname, string):
    with open(fname, 'w') as f:
        f.write(string)

def fig_gen(fname, title):
    plt.clf()
    plt.title(title)
    plt.plot(*read_2d_csv(fname))
    plt.savefig(fname + ".pdf")

# Visual debug tool for 3d
def plotf(f, xyzs_, start=np.array([-1,-1]), end=np.array([1,1]), smoothness=1.0, autobound=True, autosmooth=True):
    fig = plt.figure()
    ax = fig.gca(projection='3d')

    z, xys = zip(*xyzs_) # prepare to draw the line
    x, y = zip(*xys)
    N = len(x)
    for i in xrange(N-1):
        ax.plot(x[i:i+2], y[i:i+2], z[i:i+2], color=plt.cm.jet(255*i/N))

    if autobound:
        start = reduce(np.minimum, xys)
        end = reduce (np.maximum, xys)

    if autosmooth:
        smoothness = norm(end - start) / 100.0

    print(start, end, smoothness)

    xs = np.arange(start[0], end[0], smoothness)
    ys = np.arange(start[1] + y[-1], end[1] + y[-1], smoothness)
    X, Y = np.meshgrid(xs, ys)
    zs = np.array([f((x,y)) for x,y in zip(np.ravel(X), np.ravel(Y))])
    Z = zs.reshape(X.shape)
    surf = ax.plot_surface(X, Y, Z, rstride=1, cstride=1, cmap=cm.coolwarm, # draw the surface
            linewidth=0, antialiased=True)
    fig.colorbar(surf, shrink=0.5, aspect=5)

    plt.show()

def perf_vs_args(f, algo, dim, scale, iters, argf, target, m=100):
    """ Evaluate the performance of the given optimizer as a function of its arguments.
        Note that the dimension and scale cannot be abstracted over because they are not actually arguments to the optimizer, but rather to the
        underlying optimizer 'constructor constructor', e.g. random_dm, passed as the algo parameter.

        Arguments:
            argf     -- unary function of values in iters that should product a dict of arguments
                        to be expanded into the construction of the optimizer.
        """
    dats = []
    for i in iters:
        d = optimization_stats(*optimizer_test(f, algo, dim, scale, m, opt_args=argf(i)), target=0.0, success_threshold=0.001)
        dats.append( (i,) + d )
        print(dats[-1])
    return dats


def perf_vs_iters(f, algo, dim, scale, iters, niter_name='max_iterations', m=100, opt_args={}, target=0.0):
    """ Measure average performance versus maximum number of iterations. """
    dats = []
    for i in iters:
        d = optimization_stats(*optimizer_test(f, algo, dim, scale, m, opt_args=dict(opt_args.items() + [(niter_name, i)])),
                               target=0.0, success_threshold=0.001)
        dats.append( (i,) + d )
        print(dats[-1])
    return dats

def dm_perf_vs_refresh_rate(f, d, scale, iters, refresh_rates, m=100, opt_args={'max_iterations':100}, target=0.0):
    dats = []
    for i in iters:
        d = optimization_stats(*optimizer_test(f, algo, d, scale, m, opt_args={niter_name:i}), target=0.0, success_threshold=0.001)
        dats.append( (i,) + d )
        print(dats[-1])
    return dats

statnames = ['Success rate vs. iters.', 'Average runtime vs. iters.', 'Average function evals. vs. iters.']

def dm_callback_vs_i(f, callback, color='0.6', *args, **kwargs):
    r = dm.minimize(f, *args, callback=callback, **kwargs)
    (xs, ys) = zip(*[(i, y) for (i, y) in enumerate(r.opt.vs, 1)])
    plt.plot(xs, ys, color=color)
    return list(ys), r

def dm_callback_vs_i_with_trend(f, callback, trend_color='blue', main_color='0.6', n=100, d=2, scale=64, dm_opts={}):
    yss = []
    rs  = []
    for i in xrange(n):
        ys, r = dm_callback_vs_i(f, callback, main_color, random_guess(d, scale), random_guess(d, scale), **dm_opts)
        yss.append(ys)
        rs.append(r)
    avgs = [sum(ys) / float(len(ys)) for ys in map(list, zip(*yss))]
    plt.plot(*zip(*list(enumerate(avgs, 1))), color=trend_color)
    return yss, rs

def dm_best_minimum_vs_i_with_trend():
    def record_best_minimum(self):
        self.vs.append(self.vals[0].y)

    dm_callback_vs_i_with_trend(unwrap_bench(bench.ackley), record_best_minimum, 100, 3, 64, dm_opts={'max_iterations':100})

def dm_current_minimum_vs_i_with_trend():
    def record_current_minimum(self):
        self.vs.append(self.fv)

    dm_callback_vs_i_with_trend(unwrap_bench(bench.ackley), record_current_minimum, 100, 3, 64, dm_opts={'max_iterations':100})

def dm_stepsize_vs_i_with_trend(f, d, n, scale):
    def record_stepsize(self):
        n = norm(self.step)
        if n < 100000:
            self.vs.append(n)

    dm_callback_vs_i_with_trend(f, record_stepsize, 'blue', '0.6', n, d, scale, dm_opts={'max_iterations':100})

def dm_position_vs_i_with_trend(f, n, d, scale, dm_opts={'max_iterations':100}):
    yss = []
    rs  = []
    for i in xrange(n):
        r = dm.minimize(f, random_guess(d, scale), random_guess(d, scale), **dm_opts)
        yss.append(map(lambda x: x[0], r.lpos))
        rs.append(r)
        plt.plot(*zip(*list(enumerate(yss, 1))), color='0.6')
    avgs = [sum(ys) / float(len(ys)) for ys in map(list, zip(*yss))]
    plt.plot(*zip(*list(enumerate(avgs, 1))), color='blue')

def stats_vs_N(f, opt, dims, scale, target, success_threshold, n=100, opt_args={}):
    dats = []
    for d in dims:
        stats = optimization_stats(*optimizer_test(f, opt, d, scale, n, opt_args), target=target, success_threshold=success_threshold)
        dats.append( (d,) + stats )
    return dats

def dm_stats_vs_maxiters(f, d, maxiters, scale, target, success_threshold, n=100):
    dats = []
    for iters in maxiters:
        stats = optimization_stats(*optimizer_test(f, random_dm, d, scale, n, opt_args={'max_iterations':iters}), target=target,
                success_threshold=success_threshold)
        dats.append( (iters,) + stats )
    return dats

def dm_success_rate_vs_distance_from_minimum(f, n=100, dm_opts={}):
    def random_unit_vector(dim):
        v = []
        [v.append(random.uniform(0, 1)) for i in xrange(dim)]
        vec = np.array(v)
        return vec / norm(vec) # the division distributes to every element in the array

    def random_dm(f, d, scale, dm_args={}):
        return dm.minimize(f, scale * random_unit_vector(d), scale * random_unit_vector(d), **dm_args)

    dats = []
    for scale in xrange(2, 150, 2):
        dats.append((scale,) + optimization_stats(*optimizer_test(unwrap_bench(bench.ackley), random_dm, 2, scale, n=n, opt_args=kwargs),
                                              target=0.0, success_threshold=0.001)),
    return dats

def multiplot(dats, names=[], nrows=None, ncols=1):
    """ Make multiple plots in the case where each x value has several y values associated with it.
        If nrows is None, then the number of rows is calculated based on the size of the tuples in dats and
        the number of columns specified. """

    if len(dats) == 0:
        raise ValueError("No data.")

    if len(dats[0]) == 1:
        raise ValueError("Can't plot a 1D object.")

    if len(names) != 0 and len(names) != len(dats[0]) - 1:
        raise ValueError("Incorrect number of names given for subplots.")

    if nrows is None:
        nrows = int(np.ceil((len(dats[0]) - 1) / float(ncols)))

    plt.subplots(nrows=nrows, ncols=ncols)
    plt.tight_layout()

    for i in xrange(1, len(dats[0])):
        plt.subplot(nrows, ncols, i)
        if names:
            plt.title(names[i-1])
        plt.plot(*zip(*map(lambda dat: (dat[0], dat[i]), dats)))

minimization = 1
maximization = -1

tests = map(lambda xs: dict(zip(["name", "function", "optimization_type", "dimensions", "range", "optimum"], xs)),
        [("ackley", unwrap_bench(bench.ackley), minimization, None, (-15, 30), 0),
        ("cigar", unwrap_bench(bench.cigar), minimization, None, None, 0),
        ("sphere", unwrap_bench(bench.sphere), minimization, None, None, 0),
        ("bohachevsky", unwrap_bench(bench.bohachevsky), minimization, None, (-100, 100), 0),
        ("griewank", unwrap_bench(bench.griewank), minimization, None, (-600, 600), 0),
        ("h1", unwrap_bench(bench.h1), maximization, 2, (-100, 100), 2),
        ("himmelblau", unwrap_bench(bench.himmelblau), minimization, 2, (-6, 6), 0),
        ("rastrigin", unwrap_bench(bench.rastrigin), minimization, None, (-5.12, 5.12), 0),
        ("rosenbrock", unwrap_bench(bench.rosenbrock), minimization, None, None, 0),
        ("schaffer", unwrap_bench(bench.schaffer), minimization, None, (-100, 100), 0),
        ("schwefel", unwrap_bench(bench.schwefel), minimization, None, (-500, 500), 0)])
        ## The following functions are 'weird' in some way that makes testing too difficult.
        #(unwrap_bench(bench.rastrigin_scaled),
        #(bench.rastrigin_skew
        #(unwrap_bench(bench.rand), None, None, None, None),
        #(unwrap_bench(bench.plane), minimization, None, None, 0),

# we're going to stop naming things after dates, due to the new commit-enforcing policy.
def dm_poll_callback(self):
    self.vs.append( (self.fv, self.vals[0].y, norm(self.step)) )
poll_names = ["function_value", "best_minimum", "step_size"]
defaults = {"dimensions":2, "range":(-100, 100), "refresh_rate":4, "max_iterations":250, "runs":250, "success_threshold":0.001,
                 "callback":dm_poll_callback}

def conduct_experiment(test, defaults):
    runs         = defaults["runs"]
    dimensions   = test["dimensions"] or defaults["dimensions"]
    range        = test["range"]      or defaults["range"]
    refresh_rate = defaults["refresh_rate"]

    rs = [] # collect the OptimizeResult objects in here.
    start_time = time()
    for i in xrange(runs):
        #import pdb; pdb.set_trace()
        rs.append(dm.minimize(test["function"], randomr_guess(dimensions, range),
            randomr_guess(dimensions, range), max_iterations=defaults["max_iterations"],
            refresh_rate=defaults["refresh_rate"], callback=defaults["callback"]))
    end_time   = time()

    time_total    = end_time - start_time
    time_avg      = time_total / float(runs)
    nfev_total    = sum(imap(lambda r: r.opt.nfev, rs))
    nfev_avg      = nfev_total / float(runs)
    success_total = len(filter(lambda r: r.success and abs(r.lpos[-1][0] - test["optimum"]) < defaults["success_threshold"], rs))
    success_rate  = success_total / float(runs)
    failures      = len(filter(lambda r: not r.success, rs))

    return (failures, success_rate, time_avg, nfev_avg, rs)

def calculate_averages(statistics): # [[[a]]] -> [[a]]
    """ Take a [[[a]]], where [a] is the period sampling of a datum each iteration, [[a]] is such a sampling done on many individual runs,
        and finally [[[a]]] is a list of such statistics, and produce a [[a]] the same length as the input [[[a]]] that is a a list of averages
        w.r.t. time.
        """
    def avg_vs_t(s):
# take in the list of runs and their courses, and line them up
        s_ = zip(*s) # the first element of this thing is the list of values of the variable at the first iteration
        # for each list in s_, we compute the average, making a list of averages. That is the progression of the average value over time.
        return map(lambda x: sum(x) / float(len(x)), s_)
    return map(avg_vs_t, statistics)

def write_experiment_data(exp_dir, complete_experiment):
    """ Expects a tuple in the form (name, averages, all runs), and writes out all the data for this experiment into directory. """
    for (name, average_vs_t, data) in complete_experiment:
        with open(exp_dir + "/" + name + ".txt", 'w') as f:
            for (i, run_i) in enumerate(zip(*data)):
                #f.write(str(i) + ',') # writing the iteration number is not useful, since they're in order. Therefore, line # = iteration #.
                f.write(str(average_vs_t[i]))
                for value in run_i:
                    f.write(',' + str(value))
                f.write('\n')

def write_experiment_messages(exp_dir, rs):
    with open(exp_dir + '/' + "messages.txt", 'w') as f:
        map(lambda r: print(*tuple(r.message), sep='||', file=f), rs)

# statistics measured: success rate, average runtime, average function evals, function value vs time, best minimum vs time, stepsize vs time
def conduct_all_experiments(defaults=defaults, names=poll_names):
    all_statistics = [] # we will collect the general statistics for each experiment here, to perform a global average over all experiments.
    global_statistics = []

    if not os.path.exists("experiments"):
        os.makedirs("experiments")
    edir = "experiments/" + str(datetime.now())
    os.makedirs(edir)

    with open(edir + '/' + "averages.txt", 'w', 1) as f:
        start_time = time()
        print_csv("test", "success rate", "average time", "average fun. evals.", "failures", file=f)
        for test in tests:
            print("Conducting experiment:", test["name"])
            # prepare the experiment directory
            exp_dir = edir + "/" + test["name"]
            os.makedirs(exp_dir)

            # Perform the experiment, rs is the actual OptimizeResult objects
            (failures, success_rate, time_avg, nfev_avg, rs) = conduct_experiment(test, defaults);
            # extract the vs list from each result; it contains those data that fluctuate over time. We transpose this list of lists to
            # line up all the data for a given iteration
            test_data = zip(*imap(lambda r: r.opt.vs, rs)) # [([a],[a],[a])] -> ([[a]], [[b]], [[c]])

            all_statistics.append( (success_rate, time_avg, nfev_avg, failures) )
            avgs = calculate_averages(test_data)
            complete_data = zip(names, avgs, test_data)

            # print the general test data to the common file
            print_csv(test["name"], success_rate, time_avg, nfev_avg, failures, file=f)

            # print the test-specific data to its own directory.
            write_experiment_data(exp_dir, complete_data)
            write_experiment_messages(exp_dir, rs)

        ## calculate the global statistics
        # transpose the list of statistics, and calculate the averages.
        global_statistics = tuple(map(lambda stat: sum(stat) / float(len(stat)), zip(*all_statistics)))
        # record the data
        print_csv("AVERAGE", *global_statistics, file=f)
        end_time = time()

    with open(edir + '/' + "time.txt", 'w') as f:
        print(end_time - start_time, file=f)

    return edir






